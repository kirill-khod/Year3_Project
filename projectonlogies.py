# -*- coding: utf-8 -*-
"""projectonlogies.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r4Y8TLTdkT6jh3WfOLC4xRl_PlsrnqKQ
"""

!pip install pymorphy2

!wget https://github.com/buriy/spacy-ru/releases/download/v2.3_beta/ru2_combined_400ks_96.zip

!unzip ru2_combined_400ks_96.zip

!python -m spacy download ru_core_news_sm

# import spacy
# #import ru_core_news_sm
# import re
# import string
# from spacy.lang.ru.examples import sentences 
# sample_sentences = "Привет Миру! Как твои дела? Сегодня неплохая погода."
# if __name__ == '__main__':
#     #nlp = spacy.load('ru2')
#     nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)
#     doc = nlp(sample_sentences)
#     for s in doc.sents:
#     	print(list(['lemma "{}" from text "{}"'.format(t.lemma_, t.text) for t in s]))

# import spacy
# from spacy.lang.ru.examples import sentences 

# nlp = spacy.load("ru2_combined_400ks_96")
# doc = nlp(sentences[0])
# print(doc.text)
# for token in doc:
#     print(token.text, token.pos_, token.dep_)

import ru2_combined_400ks_96

class HearstPatterns(object):
  def __init__(self):

        self.__adj_stopwords = [
            'able', 'available', 'brief', 'certain',
            'different', 'due', 'enough', 'especially', 'few', 'fifth',
            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',
            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',
            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',
            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',
            'particular', 'past', 'possible', 'present', 'proud', 'recent',
            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure'
        ]

        self.__hearst_patterns = [
            (
                '(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)',
                'first'
            ),
            (
                '(such NP_\\w+ (, )?as (NP_\\w+ ?(, )?(and |or )?)+)',
                'first'
            ),
            (
                '((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)',
                'last'
            ),
            (
                '(NP_\\w+ (, )?include (NP_\\w+ ?(, )?(and |or )?)+)',
                'first'
            ),
            (
                '(NP_\\w+ (, )?e.g. (NP_\\w+ ?(, )?(and |or )?)+)',
                'first'
            ),
            (
                '(NP_\\w+ (, )?especially (NP_\\w+ ?(, )?(and |or )?)+)',
                'first'
            ),
            (
                '(NP_\\w+ (, )?other than (NP_\\w+ ? (, )?(and |or )?)+)',
                'first'
            ),
            (
                '(NP_\\w+ (, )?вроде (NP_\\w+ ?(, )?(и |или )?)+)',
                'first'
            )
            
        ]

        self.__hearst_patterns.extend([
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?any other NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?some other NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?be a NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?like (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    'such (NP_\\w+ (, )?as (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?like other NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?one of the NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?one of these NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?one of those NP_\\w+)',
                    'last'
                ),
                (
                    'example of (NP_\\w+ (, )?be (NP_\\w+ ? '
                    '(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?be example of NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?for example (, )?'
                    '(NP_\\w+ ?(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?which be call NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?which be name NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?mainly (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?mostly (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?notably (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?particularly (NP_\\w+ ? '
                    '(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?principally (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?in particular (NP_\\w+ ? '
                    '(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?except (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?other than (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?e.g. (, )?(NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ \\( (e.g.|i.e.) (, )?(NP_\\w+ ? (, )?(and |or )?)+'
                    '(\\. )?\\))',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?i.e. (, )?(NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and|or)? a kind of NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and|or)? kind of NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and|or)? form of NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?which look like NP_\\w+)',
                    'last'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?which sound like NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?which be similar to (NP_\\w+ ? '
                    '(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?example of this be (NP_\\w+ ? '
                    '(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?type (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )? NP_\\w+ type)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?whether (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(compare (NP_\\w+ ?(, )?)+(and |or )?with NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?compare to (NP_\\w+ ? (, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '(NP_\\w+ (, )?among -PRON- (NP_\\w+ ? '
                    '(, )?(and |or )?)+)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and |or )?as NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )? (NP_\\w+ ? (, )?(and |or )?)+ '
                    'for instance)',
                    'first'
                ),
                (
                    '((NP_\\w+ ?(, )?)+(and|or)? sort of NP_\\w+)',
                    'last'
                ),
                (
                    '(NP_\\w+ (, )?which may include (NP_\\w+ '
                    '?(, )?(and |or )?)+)',
                    'first'
                )
            ])

        self.__spacy_nlp = spacy.load('ru2_combined_400ks_96')

  def chunk(self, rawtext):
        doc = self.__spacy_nlp(rawtext)
        chunks = []
        for sentence in doc.sents:
            sentence_text = sentence.lemma_
            for chunk in sentence.noun_chunks:
                if chunk.lemma_.lower() == "example":
                    start = chunk.start
                    pre_token = sentence[start - 1].lemma_.lower()
                    post_token = sentence[start + 1].lemma_.lower()
                    if start > 0 and\
                            (pre_token == "for" or post_token == "of"):
                        continue
                if chunk.lemma_.lower() == "type":
                    continue
                chunk_arr = []
                replace_arr = []
                # print("chunk:", chunk)
                for token in chunk:
                    if token.lemma_ in self.__adj_stopwords + ["i.e.", "e.g.", "вроде"]:
                        continue
                    chunk_arr.append(token.lemma_)
                    # Remove punctuation and stopword adjectives
                    # (generally quantifiers of plurals)
                    if token.lemma_.isalnum():
                        replace_arr.append(token.lemma_)
                    else:
                        replace_arr.append(''.join(
                            char for char in token.lemma_ if char.isalnum()
                        ))
                if len(chunk_arr) == 0:
                    chunk_arr.append(chunk[-1].lemma_)
                chunk_lemma = ' '.join(chunk_arr)
                # print(chunk_lemma)
                replacement_value = 'NP_' + '_'.join(replace_arr)
                if chunk_lemma:
                    sentence_text = re.sub(r'\b%s\b' % re.escape(chunk_lemma),
                                           r'%s' % replacement_value,
                                           sentence_text)
            chunks.append(sentence_text)
        return chunks

  # def find_hyponyms(self, rawtext):

  #     hyponyms = []
  #     np_tagged_sentences = self.chunk(rawtext)

  #     for sentence in np_tagged_sentences:
  #         # two or more NPs next to each other should be merged
  #         # into a single NP, it's a chunk error

  #         for (hearst_pattern, parser) in self.__hearst_patterns:
  #             matches = re.search(hearst_pattern, sentence)
  #             if matches:
  #                 match_str = matches.group(0)

  #                 nps = [a for a in match_str.split() if a.startswith("NP_")]

  #                 if parser == "first":
  #                     general = nps[0]
  #                     specifics = nps[1:]
  #                 else:
  #                     general = nps[-1]
  #                     specifics = nps[:-1]

  #                 for i in range(len(specifics)):
  #                     pair = (
  #                         self.clean_hyponym_term(specifics[i]),
  #                         self.clean_hyponym_term(general)
  #                     )
  #                     # reduce duplicates
  #                     if pair not in hyponyms:
  #                         hyponyms.append(pair)

  #     return hyponyms

  # def clean_hyponym_term(self, term):
  #       # good point to do the stemming or lemmatization
  #       return term.replace("NP_", "").replace("_", " ")

h = HearstPatterns()

# hyps_rus=h.chunk("Фрукт вроде яблоко, банан, апельсин и персик.")
# hyps_rus

hyps8 = h.find_hyponyms("animal other than dogs such as cats")
hyps8

hyps4 = h.find_hyponyms("common law countries include Canada, Australia, and England enjoy toast.")
hyps4

hyps7 = h.find_hyponyms("Fruits e.g. apples, bananas, oranges and peaches.")
hyps7

text = []
text.append("Fruits e.g. apples, bananas, oranges and peaches.")
text.append("Common law countries include Canada, Australia, and England enjoy toast")
text[1]

hyps8 = h.find_hyponyms("companies such as IBM, Nokia, Proctor and Gamble")
hyps8

G = {(x,y),5}

def SuperConceptDetection(X,Y,G,e,threshold):
  # X - список кандидатов в суперконцепты (категория)
  # Y - субконцепт (товар)
  n = 0 # количество пар
  nX1 = 0 # из количества кандидатов в суперконцепты выбираем два X1- суперконцепт
  nX2 = 0 # количество пар где X2 - суперконцепт
  prodX1 = 1
  prodX2 = 1
  for key in G:
    n += G[key]
  for key in G:
    if key[0] == X[0]: 
      nX1+=G[key]
    if key[0] == X[1]:
      nX2+=G[key]
  
  for y in Y:
    nY_X1=0
    nY_X2=0
    for key in G:
      if key[0] == X[0] and key[1] == y:
        nY_X1+=1 # количество пар где Х1 - суперконцепт, зная что Y - субконцепт
      if key[0] == X[1] and key[1] == y:
        nY_X2+=1
    if nY_X1 == 0:
      prodX1 *= e
    else:
      prodX1 *= (nY_X1/nX1)
    if nY_X2 == 0:
      prodX2 *= e
    else:
      prodX2 *= (nY_X2/nX2)

  pX1 = nX1/n  
  pX2 = nX2/n
  if (pX1*prodX1)/(pX2*prodX2) > threshold:
    return X[0] # верная категория либо X1
  else:
    return X[1] # верная категория либо X2

def SubConceptDetection(x,Y,G,e):
    for key in G:
      nY=0
      for y in Y:
        if key[1]==0:
          nY+=1

X = []
Y = []
G = {}
x,y =''
for s in text:
  X.clear()
  Y.clear()
  hyps = h.find_hyponyms(s)
  for a in hyps:
    X.append(a[0])
    Y.append(a[1])
    if len(X) > 1:
      X = SuperConceptDetection(X,Y,G,e,threshold)
    if len(X) == 1:
      #sub concept detection
    if (x,y) in G:
      G[(x,y)]+=1
    else:
      G[(x,y)]=1